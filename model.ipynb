{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Sources table\n",
    "src = [\"amazon\", \"yelp\", \"imdb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    \"sentence\":[],\n",
    "    \"label\":[],\n",
    "}\n",
    "for sr in src:\n",
    "    with open(\"dataset\\\\\"+sr+\"_labelled.txt\",\"r\") as f:\n",
    "        full_text = f.read().split(\"\\n\")\n",
    "        for text in full_text:\n",
    "            try:\n",
    "                sent, label = text.split(\"\\t\")\n",
    "                data[\"sentence\"].append(sent)\n",
    "                data[\"label\"].append(int(label))\n",
    "            except:\n",
    "                print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
       " 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentence\"][0], data[\"label\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONVERT THE DICTIONARY TO DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean        0.500000\n",
       "std         0.500083\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.500000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKING THE LENGTH OF EACH SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    82\n",
       "1    27\n",
       "2    22\n",
       "3    79\n",
       "4    17\n",
       "Name: length, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"length\"] = df[\"sentence\"].apply(len)\n",
    "df[\"length\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3000.000000\n",
       "mean       65.277000\n",
       "std        44.170548\n",
       "min         7.000000\n",
       "25%        33.000000\n",
       "50%        55.500000\n",
       "75%        88.000000\n",
       "max       479.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"length\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LET'S PLOT THE LENGTH COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2c185af7988>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"length\"].plot(kind=\"hist\", bins=479)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so way plug us unless i go converter \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df[\"sentence\"])):\n",
    "    # 1. Removing Stopwords\n",
    "    temp = \" \".join([word for word in df[\"sentence\"][i].split(\" \") if word not in stopwords.words(\"english\")])\n",
    "    # 2. Removing Punctuations\n",
    "    temp = re.sub(r\"[^\\w\\s]\",\" \", temp)\n",
    "    # 3. Converting the sentence to lowecase\n",
    "    temp = temp.lower()\n",
    "    # 4. Stemming or lemmatizing\n",
    "    temp = \"\".join([ps.stem(word) for word in temp])\n",
    "    temp = \"\".join([lem.lemmatize(word) for word in temp])\n",
    "    # 4. Replacing the original sentence with the above text processing procedures\n",
    "    df[\"sentence\"][i] = temp\n",
    "print(df[\"sentence\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING WORD VECTORIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cv = CountVectorizer(max_features=500)\n",
    "X = cv.fit_transform (df[\"sentence\"]).toarray()\n",
    "y = df[\"label\"].values\n",
    "\"\"\"\n",
    "tf_idf = TfidfVectorizer(max_features=2000)\n",
    "tf_idf.fit(df[\"sentence\"])\n",
    "X = tf_idf.transform(df[\"sentence\"]).toarray()\n",
    "y = df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0]), type(X), type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITTING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2980, 20)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20,\n",
    "                                                   random_state=42)\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLING THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = lr.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Trying Random Forest method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "r_clf = RandomForestClassifier(n_estimators=100,criterion=\"entropy\",\n",
    "                              random_state=42).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = lr.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trying a MLP approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_clf = MLPClassifier(random_state=42).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = nn_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Trying a DL approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping for LSTM layer\n",
    "X_train = X_train.reshape(-1, 1, 500)\n",
    "X_test = X_test.reshape(-1,1,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train[0]), y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "??keras.layers.RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.45288181\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.62217022 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.63859399\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving model to disk\n",
    "pickle.dump(r_clf, open('model.pkl',\"wb\"))\n",
    "\n",
    "# Saving the tf-idf vectors to disk\n",
    "pickle.dump(tf_idf, open(\"tf_idf.pkl\", \"wb\"))\n",
    "\n",
    "# Loading model to compare the results\n",
    "model = pickle.load(open(\"model.pkl\",\"rb\"))\n",
    "print(model.predict(X_test[0].reshape(1,-1)))\n",
    "\n",
    "a = \"Very good product indeed\"\n",
    "tf = pickle.load(open(\"tf_idf.pkl\",\"rb\"))\n",
    "X = tf.transform([a]).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
